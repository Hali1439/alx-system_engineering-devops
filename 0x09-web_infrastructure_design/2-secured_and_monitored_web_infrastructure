1. Overview
We‚Äôre hosting www.foobar.com on a three-server web infrastructure, designed for better security, encrypted traffic, and real-time monitoring.
It builds upon the previous distributed web infrastructure by adding firewalls, SSL/TLS encryption, and monitoring tools.

2. Components
1. Load Balancer (HAProxy)
Role: Distributes incoming traffic evenly to multiple backend servers.

Why it's here: Prevents any one server from getting overloaded and provides redundancy.

Setup type: Active-Active ‚Äî both backend servers handle traffic at the same time.

Algorithm: Often Round Robin or Least Connections.

SSL Termination: Decrypts HTTPS traffic before passing it to backend servers (but this has a trade-off, explained later).

2. Web & Application Servers
Two servers, each running:

Nginx (Web Server) ‚Üí Serves static content, handles requests.

Application Server (e.g., Gunicorn, uWSGI) ‚Üí Executes dynamic code.

Application files/codebase ‚Üí Same version deployed on both servers.

Why: Redundancy and load distribution.

3. Database Cluster (MySQL)
Primary (Master): Accepts write and read operations.

Replica (Slave): Accepts read-only queries.

Why: Improves performance and provides a backup in case the Primary fails.

Replication type: Asynchronous (updates to Replica may be slightly delayed).

4. Firewalls (3 total)
Why: Protect each network segment from unauthorized access.

Placement:

Between Internet and Load Balancer ‚Üí Blocks malicious traffic before it reaches LB.

Between Load Balancer and Web/Application Servers ‚Üí Only allows HTTP/HTTPS from LB.

Between Application Servers and Database ‚Üí Only allows database queries from trusted app servers.

5. SSL Certificate
Why: Encrypts data between the user‚Äôs browser and the infrastructure.

How: Ensures all traffic to www.foobar.com uses HTTPS.

Prevents:

Data interception (Man-in-the-Middle attacks)

Credential theft

Implementation: Installed on the Load Balancer for SSL termination.

6. Monitoring Clients
Why: Detect failures, bottlenecks, and performance issues early.

How:

Installed on each server.

Collects metrics (CPU usage, memory usage, QPS, response times, error rates).

Sends data to central monitoring service (e.g., Sumologic, Datadog, Prometheus).

QPS Monitoring:

Track requests per second (QPS) using logs or LB stats.

Use monitoring alerts if QPS spikes abnormally.

3. How It Works (Step-by-Step Request Flow)
User enters https://www.foobar.com.

Browser looks up DNS record (A record for www.foobar.com ‚Üí IP of Load Balancer).

Request hits Firewall 1 (Internet ‚Üí LB).

Load Balancer decrypts SSL, chooses a backend server using load-balancing algorithm.

Request passes Firewall 2 (LB ‚Üí Web/App servers).

Web Server (Nginx) serves static files or forwards to Application Server.

Application Server queries the Database if needed.

Request passes Firewall 3 (App servers ‚Üí Database).

Database sends data back to the application ‚Üí web server ‚Üí load balancer ‚Üí user (encrypted again if SSL is re-applied before sending back).

4. Benefits
Security:

Firewalls block unauthorized access.

HTTPS encrypts data.

Scalability:

Load Balancer supports adding more backend servers.

Monitoring:

Quick detection of failures and performance issues.

5. Issues
SSL Termination at Load Balancer

Traffic between LB and backend servers is unencrypted unless you enable end-to-end SSL.

Could be a problem in untrusted networks.

Single MySQL Primary Write Node

If Primary fails, writes stop until failover.

Identical Servers with All Roles

If all servers have the same stack and a security breach occurs, an attacker may gain access to all components.
Issues with This Infrastructure
‚ò†Ô∏è 1. Single Points of Failure (SPOF)
Even though the system has redundancy in some areas, there are still weak links:

Load Balancer ‚Äì If it fails, the entire site goes down because all traffic depends on it.

Primary Database ‚Äì Only one MySQL node accepts writes. If it fails, no new data can be added until failover.

Monitoring Service Dependency ‚Äì If the monitoring tool is cloud-based and its agent fails or loses connection, you might miss alerts.

üîì 2. Security Gaps
SSL Termination at Load Balancer ‚Äì Traffic between the Load Balancer and backend servers is unencrypted unless you set up end-to-end SSL.

Identical Full-Stack Servers ‚Äì If each server runs both the web app and other services without separation, a compromise on one could give attackers access to multiple layers.

Potential Firewall Misconfiguration ‚Äì One bad firewall rule could accidentally open sensitive ports to the public.

üõë 3. Database Limitations
Write Bottleneck ‚Äì Only the Primary handles writes, creating a potential performance bottleneck.

Replication Lag ‚Äì In Primary‚ÄìReplica setups, replicas may be slightly behind, causing stale reads.

Failover Complexity ‚Äì Switching to a Replica as Primary requires manual or automated intervention, which takes time and may cause downtime.

üìä 4. Monitoring Blind Spots
Agent Failures ‚Äì If a monitoring client crashes, metrics from that server stop being collected.

No Synthetic Monitoring ‚Äì Monitoring might track server health but not simulate real user interactions.

Delayed Alerts ‚Äì Some monitoring tools batch alerts, causing slow response to urgent issues.

üí• 5. Scaling Challenges
Database Scaling ‚Äì Adding more replicas helps reads, but write scalability still depends on one node.

Load Balancer Limits ‚Äì One load balancer may eventually become overloaded if traffic spikes massively.

Vertical Scaling Risk ‚Äì If you only scale by upgrading servers instead of adding more, costs and risks increase.

‚ö†Ô∏è 6. Operational Risks
No Disaster Recovery Plan ‚Äì Backups aren‚Äôt mentioned; if data corruption happens, recovery could be slow or impossible.

Maintenance Downtime ‚Äì Taking down the Primary DB or LB for maintenance affects the whole site.

No Auto-Healing ‚Äì If a server fails, there‚Äôs no system in place to replace it automatically.
